{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aaf6d1c",
   "metadata": {},
   "source": [
    "# LingualLens: Multi-Provider Model Support Demo\n",
    "\n",
    "This notebook demonstrates using models from Hugging Face, OpenAI, Google, and Anthropic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lingualens import LingualLens  # Main library interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccb91b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll initialize the LingualLens with API keys for different providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LingualLens with different provider API keys\n",
    "lens = LingualLens(\n",
    "    openai_api_key=\"YOUR_OPENAI_API_KEY\",  # For OpenAI models\n",
    "    hf_api_key=\"YOUR_HUGGINGFACE_API_KEY\",  # For Hugging Face models\n",
    "    google_api_key=\"YOUR_GOOGLE_API_KEY\",  # For Google models\n",
    "    anthropic_api_key=\"YOUR_ANTHROPIC_API_KEY\"  # For Claude models\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b1b0e",
   "metadata": {},
   "source": [
    "## Using OpenAI Models\n",
    "\n",
    "Let's start with OpenAI's GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using OpenAI\n",
    "response = lens.generate(\n",
    "    prompt=\"Explain the concept of transfer learning in AI.\",\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fcbd9b",
   "metadata": {},
   "source": [
    "## Using Hugging Face Models\n",
    "\n",
    "Now let's try using a model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9327ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using Hugging Face\n",
    "response = lens.generate(\n",
    "    prompt=\"Write a short poem about machine learning.\",\n",
    "    provider=\"huggingface\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96537cf",
   "metadata": {},
   "source": [
    "## Using Anthropic's Claude\n",
    "\n",
    "Next, we'll try Anthropic's Claude model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a97c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using Anthropic\n",
    "response = lens.generate(\n",
    "    prompt=\"Compare and contrast supervised and unsupervised learning.\",\n",
    "    provider=\"anthropic\",\n",
    "    model=\"claude-3-opus-20240229\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ac365",
   "metadata": {},
   "source": [
    "## Using Google's Gemini\n",
    "\n",
    "Finally, let's try Google's Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c2d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text using Google\n",
    "response = lens.generate(\n",
    "    prompt=\"Explain how transformers work in deep learning.\",\n",
    "    provider=\"google\",\n",
    "    model=\"gemini-pro\"\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bae1c",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare the responses from different providers on the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e64315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all providers with the same prompt\n",
    "prompt = \"What are the ethical considerations of AI?\"\n",
    "\n",
    "results = {}\n",
    "providers = {\n",
    "    \"openai\": \"gpt-3.5-turbo\",\n",
    "    \"huggingface\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"anthropic\": \"claude-3-opus-20240229\",\n",
    "    \"google\": \"gemini-pro\"\n",
    "}\n",
    "\n",
    "for provider, model in providers.items():\n",
    "    print(f\"Generating response from {provider} ({model})...\")\n",
    "    results[provider] = lens.generate(\n",
    "        prompt=prompt,\n",
    "        provider=provider,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "# Display results in a dataframe for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Provider\": list(results.keys()),\n",
    "    \"Model\": [providers[p] for p in results.keys()],\n",
    "    \"Response\": list(results.values())\n",
    "})\n",
    "\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba25cc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use LingualLens with multiple model providers:\n",
    "\n",
    "1. OpenAI (GPT models)\n",
    "2. Hugging Face (various open-source models)\n",
    "3. Anthropic (Claude models)\n",
    "4. Google (Gemini models)\n",
    "\n",
    "This multi-provider approach gives you flexibility to choose the best model for your specific use case, compare results across providers, and build applications that are not dependent on a single vendor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
